Metadata-Version: 2.4
Name: seadronessee-mot
Version: 0.1.0
Summary: SeaDronesSee Multiple Object Tracking Project
Author-email: Your Name <your.email@example.com>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: duckdb>=1.3.2
Requires-Dist: pandas>=2.3.2
Requires-Dist: pyarrow>=21.0.0
Requires-Dist: opencv-python>=4.12.0
Requires-Dist: numpy>=2.2.6
Requires-Dist: faiss-cpu>=1.12.0
Requires-Dist: torch>=2.8.0
Requires-Dist: torchvision>=0.23.0
Requires-Dist: open-clip-torch>=3.1.0
Requires-Dist: tqdm>=4.67.1
Requires-Dist: streamlit>=1.49.1

# SeaDronesSee MOT Project

A comprehensive Multiple Object Tracking (MOT) project for the SeaDronesSee dataset, optimized for re-identification (re-ID) and cross-video association using embeddings and vector search.

## 🎯 Project Overview

This project processes the SeaDronesSee MOT dataset to create efficient parquet files optimized for:
- **Re-identification (re-ID)**: Find same objects across different videos
- **Cross-video association**: Link objects across multiple videos
- **Vector similarity search**: Efficient embedding-based object matching
- **Temporal consistency**: Maintain object identity over time

## 🚀 Quick Start

### 1. Setup the Project
```bash
# Interactive setup (prompts for cleanup)
./scripts/setup.sh

# Clean setup (removes old environment without prompting)
./scripts/setup.sh --clean

# Force recreation of virtual environment
./scripts/setup.sh --force
```

### 2. Convert Data
```bash
# Convert training data to parquet
./scripts/convert_json_to_parquet.sh

# Clean and convert
./scripts/convert_json_to_parquet.sh --clean
```

### 3. Clean Up (Optional)
```bash
# Clean everything
./scripts/cleanup.sh --all

# Clean specific components
./scripts/cleanup.sh --env      # Virtual environment
./scripts/cleanup.sh --cache    # Cache files
./scripts/cleanup.sh --logs     # Log files
./scripts/cleanup.sh --parquet  # Parquet files
```

## 📁 Project Structure

```
SeaDronesSee_MOT/
├── .venv/                    # Virtual environment
├── data/
│   ├── annotations/          # JSON annotation files
│   │   ├── instances_train_objects_in_water.json
│   │   ├── instances_val_objects_in_water.json
│   │   └── instances_test_objects_in_water.json
│   └── parquet/             # Converted parquet files (optimized for re-ID)
│       ├── categories.parquet
│       ├── videos.parquet
│       ├── images.parquet/   # Partitioned by video_id
│       ├── annotations.parquet/ # Partitioned by category_id + track_id
│       └── tracks.parquet/   # Partitioned by track_id
├── logs/                    # Log files
├── scripts/                 # Utility scripts
│   ├── setup.sh
│   ├── cleanup.sh
│   └── convert_json_to_parquet.sh
└── src/                     # Source code
    ├── convert_to_parquet/  # Data conversion modules
    ├── duckdb/             # Database schemas
    └── parquet/            # Parquet processing
```

## 🔧 Available Scripts

### 1. `setup.sh` - Project Setup
Sets up the virtual environment and installs dependencies using uv.

**Usage:**
```bash
./scripts/setup.sh [OPTIONS]

Options:
  --clean     Clean up old environment without prompting
  --force     Force recreation of virtual environment
  --help, -h  Show help message
```

**What it does:**
- Checks for uv installation
- Cleans up old virtual environment (optional)
- Creates new virtual environment using uv
- Installs dependencies from pyproject.toml
- Verifies installation by testing key packages

### 2. `cleanup.sh` - Project Cleanup
Cleans up various project files and directories.

**Usage:**
```bash
./scripts/cleanup.sh [OPTIONS]

Options:
  --all       Clean everything (virtual env, cache, logs, parquet)
  --env       Clean only virtual environment and lock files
  --cache     Clean only cache files (__pycache__, .pyc)
  --logs      Clean only log files
  --parquet   Clean only parquet files
  --help, -h  Show help message
```

### 3. `convert_json_to_parquet.sh` - Data Conversion
Converts JSON annotation files to parquet format with optimized partitioning.

**Usage:**
```bash
./scripts/convert_json_to_parquet.sh [OPTIONS]

Options:
  --clean     Clean output directory before processing
  --help, -h  Show help message
```

## 📊 Data Partitioning Strategy

### Optimized for Re-ID and Cross-Video Association

| Table | Partitioning | Reasoning |
|-------|-------------|-----------|
| **Categories** | None | Small dataset (~4 categories) |
| **Videos** | None | Small dataset (~20 videos) |
| **Images** | `video_id` | Natural video grouping |
| **Annotations** | `category_id` + `track_id` | **Re-ID optimization** ⭐ |
| **Tracks** | `track_id` | Object trajectory grouping |

### Re-ID Benefits:
- **Category-based search**: Search within same object type
- **Track-level granularity**: Complete object trajectories
- **Cross-video association**: Find same objects across videos
- **Vector search efficiency**: Optimized for embedding similarity

## 🎯 Use Cases

### 1. Re-identification (re-ID)
Find the same object across different videos/frames using embeddings.

### 2. Cross-video Association
Link objects across multiple videos within the same category.

### 3. Vector Similarity Search
Efficient search within object categories for similar embeddings.

### 4. Track Analysis
Access complete object trajectories for temporal analysis.

### 5. Video-based Queries
Filter images by video_id for frame sequences.

## 📈 Dataset Statistics

Based on the training dataset:
- **Categories**: 4 (swimmer, swimmer with life jacket, boat, life jacket)
- **Videos**: 20 videos
- **Images**: 27,259 images across 21 video partitions
- **Annotations**: 160,470 annotations across category_id + track_id partitions
- **Tracks**: 323 tracks across track_id partitions

## 🔧 Technical Features

- **Colored Output**: Blue info, green success, yellow warnings, red errors
- **Error Handling**: Exits on errors with helpful messages
- **Interactive Prompts**: Asks before removing important files
- **Dependency Checks**: Validates required tools (uv)
- **Comprehensive Logging**: Detailed progress information
- **Descriptive Filenames**: No random UUIDs, clear naming convention
- **Hive Convention**: Standard partitioning folder structure

## 📋 Requirements

- **uv**: Python package manager (https://docs.astral.sh/uv/)
- **bash**: Unix shell
- **Python 3.10+**: For the project dependencies

## 🛠️ Dependencies

Core dependencies include:
- **pandas**: Data manipulation and parquet handling
- **pyarrow**: Fast parquet I/O
- **torch**: Deep learning framework
- **opencv-python**: Computer vision
- **duckdb**: Database operations
- **faiss-cpu**: Vector similarity search

## 📝 Output Files

### Parquet Files with Descriptive Names:
```
annotations_category_1_track_3.parquet
annotations_category_2_track_420.parquet
images_video_0.parquet
images_video_21.parquet
tracks_video_4.parquet
```

### Partition Structure:
```
annotations.parquet/
├── category_id=1/
│   ├── track_id=3/
│   │   └── annotations_category_1_track_3.parquet
│   └── track_id=11/
│       └── annotations_category_1_track_11.parquet
└── category_id=2/
    └── ...
```

## 🚀 Development Workflow

1. **Setup**: `./scripts/setup.sh --clean`
2. **Convert**: `./scripts/convert_json_to_parquet.sh --clean`
3. **Develop**: Work with optimized parquet files
4. **Clean**: `./scripts/cleanup.sh --all` (when done)

## 📚 Documentation

- **Scripts**: See `scripts/README.md` for detailed script documentation
- **Partitioning**: See `src/convert_to_parquet/README.md` for partitioning strategy
- **Database**: See `src/duckdb/` for database schemas

## 🤝 Contributing

1. Use the provided scripts for environment management
2. Follow the partitioning strategy for data optimization
3. Test with the training dataset first
4. Use descriptive filenames and proper error handling

## 📄 License

This project is part of the SeaDronesSee MOT dataset processing pipeline.
